{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T14:11:36.035232Z",
     "start_time": "2024-12-15T14:11:35.959412Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import deque\n",
    "import random"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:12:13.924169Z",
     "start_time": "2024-12-15T14:12:13.921170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the environment\n",
    "n_states = 16  # Number of states in the grid world\n",
    "n_actions = 4  # Number of possible actions (up, down, left, right)\n",
    "goal_state = 15  # Goal state\n",
    "\n",
    "# Define parameters\n",
    "learning_rate0 = 0.8 # Initial learning rate\n",
    "decay = 0.01  # Learning rate decay\n",
    "discount_factor = 0.95\n",
    "exploration_probability = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "#replay buffer\n",
    "buffer_size = 100  # Maximum size of the replay buffer\n",
    "batch_size = 32  # Number of samples to train on per epoch"
   ],
   "id": "b92225e62cb02d47",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:12:15.188385Z",
     "start_time": "2024-12-15T14:12:15.185471Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_table = np.zeros((n_states, n_actions))\n",
    "replay_buffer = deque(maxlen=buffer_size)"
   ],
   "id": "d3a3799d6c630651",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:12:17.303177Z",
     "start_time": "2024-12-15T14:12:16.584273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_probability:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Store transition in the replay buffer\n",
    "        replay_buffer.append((current_state, action, reward, next_state))\n",
    "\n",
    "        # Train using a random batch from the replay buffer\n",
    "        if len(replay_buffer) >= batch_size:\n",
    "            batch = random.sample(replay_buffer, batch_size)\n",
    "            for s, a, r, ns in batch:\n",
    "                learning_rate = learning_rate0 / (1 + epoch * decay)\n",
    "                Q_table[s, a] += learning_rate * \\\n",
    "                                 (r + discount_factor * np.max(Q_table[ns]) - Q_table[s, a])\n",
    "\n",
    "        current_state = next_state  # Move to the next state"
   ],
   "id": "b269f33e1f34b270",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:12:18.247739Z",
     "start_time": "2024-12-15T14:12:18.244551Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The resulting Q-Table\n",
    "print(\"\\nQ-Table after Q-Learning:\")\n",
    "print(Q_table)"
   ],
   "id": "2ceb076719149504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table after Q-Learning:\n",
      "[[0.48767498 0.48096461 0.48764259 0.48767498]\n",
      " [0.51334208 0.51334208 0.51334208 0.51334208]\n",
      " [0.54035856 0.54036009 0.54036009 0.54036009]\n",
      " [0.56880009 0.56880009 0.56880009 0.56880009]\n",
      " [0.59873694 0.59873694 0.59873694 0.59873694]\n",
      " [0.63024941 0.63024941 0.63024941 0.63024941]\n",
      " [0.66342043 0.66342043 0.66342043 0.66342043]\n",
      " [0.6983373  0.6983373  0.6983373  0.6983373 ]\n",
      " [0.73509189 0.73509189 0.73509189 0.73509189]\n",
      " [0.77378094 0.77378094 0.77378094 0.77378094]\n",
      " [0.81450625 0.81450625 0.81450625 0.81450625]\n",
      " [0.857375   0.857375   0.857375   0.857375  ]\n",
      " [0.9025     0.9025     0.9025     0.9025    ]\n",
      " [0.95       0.95       0.95       0.95      ]\n",
      " [1.         1.         1.         1.        ]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:12:26.692271Z",
     "start_time": "2024-12-15T14:12:26.689680Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimal policy\n",
    "optimal_policy = np.argmax(Q_table, axis=1)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(optimal_policy)"
   ],
   "id": "2995a0e7ff0dd103",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "[0 0 1 3 0 0 0 0 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "execution_count": 11
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
