{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T13:29:15.545117Z",
     "start_time": "2024-12-15T13:29:15.542943Z"
    }
   },
   "source": "import numpy as np",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:43:22.455792Z",
     "start_time": "2024-12-15T13:43:22.452773Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the environment\n",
    "n_states = 16  # Number of states in the grid world\n",
    "n_actions = 4  # Number of possible actions (up, down, left, right)\n",
    "goal_state = 15  # Goal state\n",
    "\n",
    "# Define parameters\n",
    "learning_rate0 = 0.8 # Initial learning rate\n",
    "decay = 0.01  # Learning rate decay\n",
    "discount_factor = 0.95\n",
    "\n",
    "exploration_probability = 0.2\n",
    "\n",
    "epochs = 1000"
   ],
   "id": "b92225e62cb02d47",
   "outputs": [],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:43:24.034069Z",
     "start_time": "2024-12-15T13:43:24.031239Z"
    }
   },
   "cell_type": "code",
   "source": "Q_table = np.zeros((n_states, n_actions))",
   "id": "d3a3799d6c630651",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:43:25.397850Z",
     "start_time": "2024-12-15T13:43:25.349300Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_probability:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        next_state = (current_state + 1) % n_states\n",
    "\n",
    "        learning_rate = learning_rate0 / (1 + epoch * decay)\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else 0\n",
    "\n",
    "        # Update Q-value using bellman-equation\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "                                          (reward + discount_factor *\n",
    "                                           np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "\n",
    "        current_state = next_state  # Move to the next state"
   ],
   "id": "b269f33e1f34b270",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:43:26.868041Z",
     "start_time": "2024-12-15T13:43:26.865224Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# The resulting Q-Table\n",
    "print(\"\\nQ-Table after Q-Learning:\")\n",
    "print(Q_table)"
   ],
   "id": "2ceb076719149504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table after Q-Learning:\n",
      "[[0.48716873 0.13572149 0.28606469 0.28084855]\n",
      " [0.33302451 0.31524609 0.26168524 0.51334189]\n",
      " [0.39878388 0.44755844 0.48421607 0.54036009]\n",
      " [0.41166687 0.44369741 0.53555674 0.56880009]\n",
      " [0.59873694 0.52737653 0.57581272 0.59462335]\n",
      " [0.63024941 0.58652047 0.53528394 0.61018997]\n",
      " [0.64461535 0.66004942 0.61872928 0.66342043]\n",
      " [0.6983373  0.69432407 0.6928497  0.69666566]\n",
      " [0.73402675 0.73130464 0.73509189 0.73268511]\n",
      " [0.77372941 0.77142092 0.76978571 0.77378094]\n",
      " [0.81450625 0.81430173 0.81286788 0.81447619]\n",
      " [0.857375   0.85702085 0.85719912 0.85561139]\n",
      " [0.9025     0.90249317 0.90249245 0.90210924]\n",
      " [0.94999747 0.94933171 0.95       0.94994173]\n",
      " [0.99999109 1.         0.99999679 0.99993717]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T13:43:30.620926Z",
     "start_time": "2024-12-15T13:43:30.617513Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Optimal policy\n",
    "optimal_policy = np.argmax(Q_table, axis=1)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(optimal_policy)"
   ],
   "id": "2995a0e7ff0dd103",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "[0 3 3 3 0 0 3 0 2 3 0 0 0 2 1 0]\n"
     ]
    }
   ],
   "execution_count": 39
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
