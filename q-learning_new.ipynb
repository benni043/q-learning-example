{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:46.254718Z",
     "start_time": "2024-12-15T14:46:46.252001Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "from collections import deque"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:47.485730Z",
     "start_time": "2024-12-15T14:46:47.482521Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Define the environment\n",
    "grid_size = 4\n",
    "n_states = grid_size * grid_size\n",
    "n_actions = 4\n",
    "goal_state = 15\n",
    "\n",
    "actions = {'up': 0, 'down': 1, 'left': 2, 'right': 3}\n",
    "\n",
    "# Define parameters\n",
    "learning_rate0 = 0.8  # Initial learning rate\n",
    "decay = 0.01  # Learning rate decay\n",
    "discount_factor = 0.95\n",
    "exploration_probability = 0.2\n",
    "epochs = 1000\n",
    "\n",
    "#replay buffer\n",
    "# buffer_size = 100  # Maximum size of the replay buffer\n",
    "# batch_size = 32  # Number of samples to train on per epoch"
   ],
   "id": "b92225e62cb02d47",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:49.173023Z",
     "start_time": "2024-12-15T14:46:49.170008Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_next_state(state, action):\n",
    "    row, col = divmod(state, grid_size)\n",
    "    if action == actions['up'] and row > 0:\n",
    "        row -= 1\n",
    "    elif action == actions['down'] and row < grid_size - 1:\n",
    "        row += 1\n",
    "    elif action == actions['left'] and col > 0:\n",
    "        col -= 1\n",
    "    elif action == actions['right'] and col < grid_size - 1:\n",
    "        col += 1\n",
    "    return row * grid_size + col"
   ],
   "id": "3aedcee4582ebcdc",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:51.528445Z",
     "start_time": "2024-12-15T14:46:51.525877Z"
    }
   },
   "cell_type": "code",
   "source": [
    "Q_table = np.zeros((n_states, n_actions))\n",
    "# replay_buffer = deque(maxlen=buffer_size)"
   ],
   "id": "d3a3799d6c630651",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:53.305382Z",
     "start_time": "2024-12-15T14:46:53.275030Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Q-learning algorithm\n",
    "for epoch in range(epochs):\n",
    "    current_state = np.random.randint(0, n_states)  # Start from a random state\n",
    "\n",
    "    while current_state != goal_state:\n",
    "        # Choose action with epsilon-greedy strategy\n",
    "        if np.random.rand() < exploration_probability:\n",
    "            action = np.random.randint(0, n_actions)  # Explore\n",
    "        else:\n",
    "            action = np.argmax(Q_table[current_state])  # Exploit\n",
    "\n",
    "        # Simulate the environment (move to the next state)\n",
    "        next_state = get_next_state(current_state, action)\n",
    "\n",
    "        # Define a simple reward function (1 if the goal state is reached, 0 otherwise)\n",
    "        reward = 1 if next_state == goal_state else -0.1\n",
    "\n",
    "        # Store transition in the replay buffer\n",
    "        # replay_buffer.append((current_state, action, reward, next_state))\n",
    "\n",
    "        # Train using a random batch from the replay buffer\n",
    "        # if len(replay_buffer) >= batch_size:\n",
    "        #     batch = random.sample(replay_buffer, batch_size)\n",
    "        #     for s, a, r, ns in batch:\n",
    "        #         learning_rate = learning_rate0 / (1 + epoch * decay)\n",
    "        #         Q_table[s, a] += learning_rate * \\\n",
    "        #                          (r + discount_factor * np.max(Q_table[ns]) - Q_table[s, a])\n",
    "\n",
    "        # Update Q-value\n",
    "        learning_rate = learning_rate0 / (1 + epoch * decay)\n",
    "        Q_table[current_state, action] += learning_rate * \\\n",
    "                                          (reward + discount_factor *\n",
    "                                           np.max(Q_table[next_state]) - Q_table[current_state, action])\n",
    "\n",
    "        current_state = next_state  # Move to the next state"
   ],
   "id": "b269f33e1f34b270",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:46:54.998624Z",
     "start_time": "2024-12-15T14:46:54.995838Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"\\nQ-Table after Q-Learning:\")\n",
    "print(Q_table)"
   ],
   "id": "2ceb076719149504",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q-Table after Q-Learning:\n",
      "[[-0.21824202  0.32133098 -0.19265715  0.14791892]\n",
      " [-0.14707673  0.1327206  -0.01980184  0.44351592]\n",
      " [ 0.27204183  0.572125    0.14646395  0.49969753]\n",
      " [ 0.26505736  0.7075      0.3806348   0.1289438 ]\n",
      " [ 0.06134118  0.44351875  0.20132995  0.01050313]\n",
      " [-0.04144797  0.4389111   0.16609809  0.572125  ]\n",
      " [ 0.37261831  0.65983416  0.4392592   0.7075    ]\n",
      " [ 0.57084391  0.85        0.57143296  0.68789369]\n",
      " [ 0.15979782  0.38240503  0.38143624  0.572125  ]\n",
      " [ 0.40625607  0.7075      0.34861786  0.65396593]\n",
      " [ 0.44971898  0.73919527  0.45023086  0.85      ]\n",
      " [ 0.7070456   1.          0.70747959  0.84991162]\n",
      " [ 0.16443156  0.27118419  0.45782219  0.70749998]\n",
      " [ 0.54086903  0.68499323  0.5690151   0.85      ]\n",
      " [ 0.70129622  0.84712669  0.69754028  1.        ]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-15T14:49:45.858035Z",
     "start_time": "2024-12-15T14:49:45.853893Z"
    }
   },
   "cell_type": "code",
   "source": [
    "optimal_policy = np.argmax(Q_table, axis=1)\n",
    "print(\"\\nOptimal Policy:\")\n",
    "print(optimal_policy)"
   ],
   "id": "2995a0e7ff0dd103",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Optimal Policy:\n",
      "[1 3 1 1 1 3 3 1 3 1 3 1 3 3 3 0]\n"
     ]
    }
   ],
   "execution_count": 20
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
